---
title: Deep Dive Into LLMs like ChatGPT 리뷰
date: 2025-04-18 13:55:17
categories:
  - "[Review, OSSCA]"
tags:
---
### 개요
[(한글자막) 대형 언어 모델(LLM)의 심층 분석: ChatGPT의 작동 방식 이해하기](https://www.youtube.com/watch?v=6PTCwRRUHjE&t=10430s)

이 영상(3시간 반)을 보면서 배웠던 내용을 내 나름대로 정리해보려고 한다.
영상은 ChatGPT와 같은 LLM이 어떻게 만들어지는지, 어떻게 작동하는지, 어떤 약점을 가지는지 등 전반적인 내용에 대해서 알려준다. 
구체적인 구현 사항들을 전부 다루진 않지만 원리를 이해하는데 많은 도움이 되었다. 꼭 보았으면 좋겠다. 시간이 없다면 적당히 나누어서 설명하기에 조금씩이라도 보는 걸 추천한다.


## LLM의 패러다임 전환 : RL기반의 추론 모델

### 강화 학습 (RL)의 가능성
지도 학습은 전문가가 답변한 내용을 "따라"할 뿐이기 때문에 아무리 학습을 반복해도 인류 최고의 전문가를 능가하기 어렵다고 한다. 반면에
강화 학습의 경우, 이세돌과 경기에서 알파고가 보여줬던 "37수"처럼 사람들이 생각하지 못한 이상적인 전략을 만들어 낼 수 있다. 이러한 관점으로 보았을 때,
기존 ChatGPT의 SFT 방식은 전문가의 답변을 따라할 뿐 그 이상을 넘기 힘들겠다는 생각을 할 수 있다. 최근 논란이 되었던 DeepSeek 모델은
이러한 방식에서 벗어나 순수 RL 기반으로 학습되어 기존 모델에 비해 우수한 성능을 낼 수 있었다.

### DeepSeek 모델의 학습 방법


### 검증하기 어려운 도메인에서의 강화학습
농담을 잘하는 모델을 만드는 예를 들어 정량적인 평가가 어려운 영역에 대해서는 강화학습을 적용하기 어렵다고 설명한다.
강화학습에는 수천 번의 업데이트가 필요하며, 한 업데이트 당 수천개의 프롬프트를 봐야하며, 한 프롬프트에는 수천개의 생성결과를 평가해야 하는데,
정량적인 평가가 어려운 영역에서는 사람이 일일이 평가를 해야하기 때문에 비용적인 문제가 커지켜 "확장 가능한" 방법이 아니다.

이를 위해 "인간 피드백 기반 강화학습"이 제시되었다.  

