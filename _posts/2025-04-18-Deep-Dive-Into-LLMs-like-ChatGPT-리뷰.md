---
title: Deep Dive Into LLMs like ChatGPT 리뷰
date: 2025-04-18 13:55:17
categories:
  - "[Review, OSSCA]"
tags:
  - "[LLM, ChatGPT, DeepSeek]"
---
### 개요
[(한글자막) 대형 언어 모델(LLM)의 심층 분석: ChatGPT의 작동 방식 이해하기](https://www.youtube.com/watch?v=6PTCwRRUHjE&t=10430s)

이 영상(3시간 반)을 보면서 배웠던 내용을 내 나름대로 정리해보려고 한다. 영상은 ChatGPT와 같은 LLM이 어떻게 만들어지는지, 어떻게 작동하는지, 어떤 약점을 가지는지 등 전반적인 내용에 대해서 알려준다. 구체적인 구현 사항들까지 다루진 않지만 적절한 비유와 함께 설명을 해주어 원리를 이해하는데 많은 도움이 되었다.

# LLM이 만들어지는 과정 

## 사전 훈련
### 데이터 셋 구성
대부분의 기업에서는 대량의 다양한 분야의 높은 질의 데이터셋을 구성하는 것을 목표로 한다. 이를 위해 많은 양의 데이터를 수집하기도 하지만, 전처리 과정이 필수적이라고 한다. 영상에서는 FineWeb 데이터셋을 예로 들어 설명하는데, 자세한 설명은 [FineWeb 데이터셋](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1)에서 볼 수 있다.대략적으로 다음과 같은 과정을 거친다고 한다.

#### FineWeb의 데이터 전처리 과정
1. URL Filtering : 성인사이트와 같은 부적절한 데이터 소스를 URL 단위로 거른다.
2. Text Extraction : HTML 문서에서 HTML 태그 정보를 제외한 텍스트 값들을 추출
3. Language Filtering : 언어 분류기를 통한 문서 내의 영어 비중을 판단하고 일정 비율 이상인 문서만 추출
4. 기타 필터링 : 개인 정보 제거 등등
> 영상에서 다뤘던 부분들만 짚고 넘어갔는데, 기타 필터링에 많은 작업들을 한다.

### 토큰화
언어 모델은 한정된 입력 크기를 가진다. 때문에 텍스트 데이터를 인코딩한 비트열을 그대로 사용하기에는 입력의 시퀀스의 길이가 필요 이상으로 길어지게 된다. 표현할 수 있는 기호를 늘려 시퀀스의 길이를 줄일 수 있다. 이를 위한 다양한 알고리즘이 있지만, 영상에서는 **바이트 페어 인코딩**이라는 알고리즘을 소개한다. 8비트를 묶은 한 바이트 단위로 표현하고, 여기서 빈번히 등장하는 바이트 페어를 묶어 새로운 토큰으로 만들어 시퀀스 길이를 줄인다. 이런 식으로 토큰 집합의 크기를 늘려 GPT 4의 경우 100,277개의 심볼을 사용한다고 한다. GPT 4의 토큰화는 [Tiktokenizer](https://tiktokenizer.vercel.app/)의 cl100k_base를 통해 체험해 볼 수 있다. 이런 토큰화는 LLM이 우리의 예상과 다르게 동작하는 기반이 된다.

## LLM의 패러다임 전환 : RL기반의 추론 모델

### 강화 학습 (RL)의 가능성
지도 학습은 전문가가 답변한 내용을 "따라"할 뿐이기 때문에 아무리 학습을 반복해도 인류 최고의 전문가를 능가하기 어렵다고 한다. 반면에
강화 학습의 경우, 이세돌과 경기에서 알파고가 보여줬던 "37수"처럼 사람들이 생각하지 못한 이상적인 전략을 만들어 낼 수 있다. 이러한 관점으로 보았을 때,
기존 ChatGPT의 SFT 방식은 전문가의 답변을 따라할 뿐 그 이상을 넘기 힘들겠다는 생각을 할 수 있다. 최근 논란이 되었던 DeepSeek 모델은
이러한 방식에서 벗어나 순수 RL 기반으로 학습되어 기존 모델에 비해 우수한 성능을 낼 수 있었다.

### DeepSeek 모델의 학습 방법


### 검증하기 어려운 도메인에서의 강화학습
농담을 잘하는 모델을 만드는 예를 들어 정량적인 평가가 어려운 영역에 대해서는 강화학습을 적용하기 어렵다고 설명한다.
강화학습에는 수천 번의 업데이트가 필요하며, 한 업데이트 당 수천개의 프롬프트를 봐야하며, 한 프롬프트에는 수천개의 생성결과를 평가해야 하는데,
정량적인 평가가 어려운 영역에서는 사람이 일일이 평가를 해야하기 때문에 비용적인 문제가 커지켜 "확장 가능한" 방법이 아니다.

이를 위해 "인간 피드백 기반 강화학습(RLHF)"이 제시되었고, 대표적으로 Rlichev 접근법이 있다.

#### Rlichev Approach
이 접근법에서는 인간의 평가를 시뮬레이션하는 보상 모델을 만들어 강화학습에 대한 평가를 사람 대신 하도록 하는 접근법이다.
물론 초기에 보상 모델을 훈련시킬 때에는 인력이 필요하지만, 수차례의 학습 후에는 이 모델로 얼마든지 결과를 평가할 수 있기 때문에 확장성 측면에서 좋다.

구체적으로 사람으로 보상 모델을 훈련시킬 때에는 다음과 같은 과정을 거친다.
1. 생성결과들을 보고 점수로 표현하기 보다는 더 쉬운 평가 방식인 "등수"를 나타내도록 한다.
2. 보상 모델에게도 {프롬프트, 후보 결과}를 주고 점수를 출력하도록 한다. (0과 1 사이)
3. 보상 모델이 출력한 결과의 등수와 사람이 정한 등수를 비교한다.
4. 수학적인 방법으로 이 차이를 줄이도록 학습시킨다. 

#### RHLF의 장점
검증 불가능한 영역에 대해서도 강화학습을 적용할 수 있다. 실제로 적용을 하면 모델 성능이 나아지는 것을 알 수 있는데 정확히 왜 그런지는 밝혀지지 않았다고 한다. 보상 모델을 훈련할 비용으로 목표하는 모델을 훈련하는게 더 직접적인 도움이 되지 않나 싶긴 하다. 영상에서는 **생성기에 비해 판별기의 복잡도가 더 낮기** 때문이라고 추측한다. 
>  이러한 비슷한 예제로 임베딩 모델을 훈련할 때 Negative Sampling 방식으로 좀 더 효율적으로 학습시켰다는 논문을 떠올릴 수 있었다. 

#### RHLF의 한계
당연히 실제 인간의 판단이 아닌 단순화 된 시뮬레이션으로 강화학습 한다는 근본적인 문제가 있지만 더 미묘한 문제는 이 방식은 모델과 시뮬레이션 자체를 오염시킬 수 있다는 점이다. 단순히 생각하기로는 만들어진 보상 모델을 이용해 학습을 오래할수록 더 높은 품질의 답변을 생성할 것 같지만, 실제로 그렇지 않다고 한다. 보상 모델은 어떤 특정한 "적대적 예시"에 대해서 실제 사람과 전혀 다른 평가를 하게 되는데, 이로 인해 이상한 출력을 높게 평가하여 결과로 만들어진 모델이 이상한 출력을 내게 될 수 있다고 한다. 
> 영상에서 제시한 예시로는 펠리컨에 대한 농담으로 "the the the the the" 이런 문장을 보상 모델이 높은 점수를 주었다고 한다.

설령 그러한 적대적 예시들을 하나하나 발견하여 데이터셋에 추가하여 조정한다고 하더라도, 강화학습의 최적화에 의해 보상 모델의 빈틈을 파고들어 무한히 많은 적대적 입력들을 만들어낼 수 있기 때문이다. 
결국 이러한 리워드 모델을 너무 많이 사용하게 되면 강화학습이 보상 모델의 약점을 찾게 되기 때문에 무한히 사용할 수 없게 된다.

영상에서는 RHLF가 무한히 사용할 수 없다는 점을 들어 진정한 RL이 아닌 미세 조정의 일종이라고 본다.


## 모델의 미래 
### Multimodal : LLM 자체에서 audio, image, video를 지원하는 모델
Audio/Iamge/Video를 토큰화하여 입력으로 사용하는 식. 근본적인 변화가 있는 것이 아니라 토큰 종류가 증가하는 것
> 스펙트로그램, 이미지 패치 등등을 이용할 수 있다.

### Tasks -> Agent
작업 실행 구성을 우리가 하고, LLM이 실행하는 현재... 긴 작업을 위해 여러 과제를 연결하는 것이 아직 불가능함.
미래에는 이러한 작업을 몇 분에서 몇 시간까지 진행하는 Agent가 등장할 것이라고 한다. 하지만 마치 자동화된 공장에서 로봇들으 관리하는 직업이 있듯이 잘 하고 있는지 볼 수 있는 감독은 필요하다. 

### Test Time Training
지금까지는 매개변수 훈련 / 추론으로 두 가지 단계가 명확하게 나누어져 있음. 모델은 상황별로 조절 가능한 컨텍스트 윈도우를 통한 문맥 학습밖에 하지 못함. 추론 과정에서 매개변수를 조정할 수 없다는 점이 아쉽다. 특히 컨텍스트 윈도우 크기는 제한적일 수 밖에 없는데, 후에 비디오와 같이 토큰이 많이 필요한 입력이 들어오게 되면 활용할 수 있는 공간이 부족하게 된다.

### 최신 발전을 경험할 수 있는 곳
1. lmarena.ai : 사람 평가 기반의 LLM 순위표, 최근 몇 달 동안은 조작된 것 같아서 예전만큼은 신뢰하지 않음 (gemini, sonnet)
2. AI News letter : 포괄적인 내용들이 많아 좋다. 대부분은 AI가 작성함. 본문 내용은 많지만 요약본은 퀄리티가 좋음.
3. X/Twitter : 신뢰할 수 있는 사람들을 팔로우 하기

### AI 모델을 접근할 수 있는 곳
1. 모델 제공자 사이트
2. Together.ai
3. Hyperbolic : base model
4. LMStudio : 작은 로컬 모델 사용하기에 적합. DeepSeek 모델을 보통 사용함.
