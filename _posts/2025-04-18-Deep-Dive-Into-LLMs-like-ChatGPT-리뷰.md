---
title: Deep Dive Into LLMs like ChatGPT 리뷰
date: 2025-04-18 13:55:17
categories: [Review, AI]
tags: [LLM, ChatGPT, DeepSeek]
---
### 개요
[(한글자막) 대형 언어 모델(LLM)의 심층 분석: ChatGPT의 작동 방식 이해하기](https://www.youtube.com/watch?v=6PTCwRRUHjE&t=10430s)

이 영상(3시간 반)을 보면서 배웠던 내용을 내 나름대로 정리해보려고 한다. 영상은 ChatGPT와 같은 LLM이 어떻게 만들어지는지, 어떻게 작동하는지, 어떤 약점을 가지는지 등 전반적인 내용에 대해서 알려준다. 구체적인 구현 사항들까지 다루진 않지만 적절한 비유와 함께 설명을 해주어 원리를 이해하는데 많은 도움이 되었다.

# LLM이 만들어지는 과정 

## 사전 훈련
### 데이터 셋 구성
대부분의 기업에서는 대량의 다양한 분야의 높은 질의 데이터셋을 구성하는 것을 목표로 한다. 이를 위해 많은 양의 데이터를 수집하기도 하지만, 전처리 과정이 필수적이라고 한다. 영상에서는 FineWeb 데이터셋을 예로 들어 설명하는데, 자세한 설명은 [FineWeb 데이터셋](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1)에서 볼 수 있다.대략적으로 다음과 같은 과정을 거친다고 한다.

#### FineWeb의 데이터 전처리 과정
1. URL Filtering : 성인사이트와 같은 부적절한 데이터 소스를 URL 단위로 거른다.
2. Text Extraction : HTML 문서에서 HTML 태그 정보를 제외한 텍스트 값들을 추출
3. Language Filtering : 언어 분류기를 통한 문서 내의 영어 비중을 판단하고 일정 비율 이상인 문서만 추출
4. 기타 필터링 : 개인 정보 제거 등등
> 영상에서 다뤘던 부분들만 짚고 넘어갔는데, 기타 필터링에 많은 작업들을 한다.

### 토큰화
이렇게 모은 데이터를 입력으로 하여 모델을 학습시켜야 하는데, 언어 모델은 한정된 입력 크기를 가진다. 
때문에 텍스트 데이터를 인코딩한 비트열을 그대로 사용하기에는 입력의 시퀀스의 길이가 필요 이상으로 길어지게 된다. 
표현할 수 있는 기호를 늘려 시퀀스의 길이를 줄일 수 있다. 이를 위한 다양한 알고리즘이 있지만, 영상에서는 **바이트 페어 인코딩**이라는 알고리즘을 소개한다. 
8비트를 묶은 한 바이트 단위로 표현하고, 여기서 빈번히 등장하는 바이트 페어를 묶어 새로운 토큰으로 만들어 시퀀스 길이를 줄인다. 
이런 식으로 토큰 집합의 크기를 늘려 GPT 4의 경우 100,277개의 토큰이 사용한다고 한다. 
GPT 4의 토큰나이저는 [Tiktokenizer](https://tiktokenizer.vercel.app/)의 cl100k_base를 통해 체험해 볼 수 있다. 이런 토큰화는 LLM이 우리의 예상과 다르게 동작하는 기반이 된다.

### 모델 학습
이렇게 토큰화 된 문서를 기반으로 다음 올 단어를 예측하는 언어모델을 만들어낸다. 일정한 크기의 컨텍스트 윈도우를 잡고 이 컨텍스트를 기반으로 다음에 올 토큰이 등장 확률 분포를
예측한다. 출력으로 토큰이 나오는 것이 아니라, 각 토큰이 나올 확률이 나온다. 실제로 문장을 만들 땐, 이 확률 분포에 따라 랜덤으로 토큰이 추출된다. 
이렇게 추출된 토큰을 기반으로 다시 다음 토큰의 확률분포를 만들어내기 때문에 운이 좋으면 원본 데이터를 만들어내지만, 원본과 완전히 다른 토큰열이 출력될 수도 있다.
> 더 잘 예측하게 하고, 더 효율적으로 학습하게 하기 위해서 RNN, LSTM, Transformer 등 다양한 모델들이 제안되었고, 현재는 Transformer 기반의 모델들이 지배적이다

수 많은 컴퓨팅 자원을 이용해 긴 시간 학습을 하고 나면 **토큰 시뮬레이터**역할을 하는 **베이스 모델**을 얻을 수 있다. 

### 베이스 모델
베이스 모델은 영상에서 소개한 것처럼 **토큰 시뮬레이터**역할을 한다. 인터넷 지식을 *상상*하며 다음에 올 토큰을 통계적으로 예측할 뿐이다. 
[Hyperbolic](https://app.hyperbolic.xyz/models)에서 베이스 모델을 테스트 해보면 입력한 문장을 계속해서 이어가기만 하는 것을 확인할 수 있다.
결과가 그렇게 유용해보이진 않지만, 다음 토큰을 예측하는 학습을 하면서 세상의 지식을 신경망 내부에 저장한다는 점에서 의미가 있다.
> 영상에서 베이스 모델을 **인터넷의 손실 압축본**, **인터넷 문서 시뮬레이터**이라고 표현하는데, 굉장히 와닿았다

프롬프트는 이 압축파일에서 지식을 꺼낼 수 있는 수단으로 활용된다. 베이스 모델에서는 마치 문자을 쓰다만 것처럼 프롬프틀 작성해서 프랑스의 대표적인 관광명소 
Top10에 대해서 지식을 얻어냈다. 이렇게 얻어낸 지식은 정확하지 않을 수 있으며 많이 나오는 지식일수록 더 잘 나오게 되는 특성을 가진다. 
실제로 위키피디아의 일부 내용을 프롬프트에 전달하면 위키피디아 내용을 거의 그대로 재생산하는 것을 볼 수 있는데, 위키피디아 데이터가 이곳저곳에서 
재사용되는 것에 기인한다. 기억효율이 인간보다 훨씬 좋은 LLM은 몇번만 보더라도 거의 같은 내용을 기억해 재현할 수 있다. 

#### 환각 현상
그렇다면 학습 데이터에 존재하지 않은 지식에 대한 프롬프트는 어떻게 완성해 완성해 나갈까? 짐작한대로 자신이 알고 있는 지식을 기반으로 최선을 다해 
문장을 완성해 나간다. 그 결과 확률적으로 그럴싸한 문장을 마치 맞는 것처럼 출력한다. 이런 현상을 **환각**현상이라고 한다.

#### 문맥 내 학습
이러한 단점에도 프롬프트를 잘 설계하면 "문맥 내 학습" 능력으로 베이스 모델을 유용하게 활용할 수 있다. 대표적으로 **퓨샷 프롬프트**를 활용하면,
실시간으로 입력 프롬프트에 있는 패턴을 분석하여 내가 원하는 동작을 수행할 수 있게 한다.

## 사후 훈련 
베이스 모델만으로는 우리의 질문에 알맞게 대답하는 Assistant 역할을 잘 해내기엔 어렵다. LLM을 Assistant 역할을 하게 하기 위해 사후 처리를 하게 되는데,
사전 훈련에 비해 훨씬 컴퓨팅 자원을 덜 소비하며 빠르게 진행된다. 질문에 대답하는 Assistant 대화의 흐름을 생각해보면, 2+2=?와 같은 질문에는 4라고 대답을 하고 해킹을 해달라고 하면
할 수 없다고도 대답한다. 이처럼 LLM에게 Assistant의 행동을 가르쳐야 하는데, LLM은 신경망일 뿐이므로 코드로는 그러한 방식을 가르칠 수 없다.
대신 행동에 맞는 예시를 만들어 학습시켜 간접적으로 가르칠 수 있다. 사전 훈련과 마찬가지 알고리즘을 사용하되 데이터셋만 바뀌는 식으로 진행된다.

### 데이터셋 확보
LLM에게 특정한 행동을 가르치기 위해 그 행동에 맞게 대답하는 실제 예시들이 필요하다. 많은 LLM 기업들은 적절한 지침을 데이터 레이블러에게 가르치고,
그 지침에 맞게 어떤 질문에 대한 대답을 인간 레이블러들이 작성하며 데이터를 확보한다. 이런 데이터셋을 통해 다시 한번 학습한 모델은 별도의 프롬프트 설계를 하지 않더라도
질문에 맞게 대답하게 된다.

### 토큰화
"대화"와 "질문/대답"이라는 구조를 LLM에게 전달하기 위해 이런 구조를 반영한 토큰화가 필요하다. 기존 데이터에는 없는 특수 토큰을 추가하여 사용자의 말과 
Assistant의 말을 구분한다. 이러한 토큰을 도입하고 충분히 학습시켜 특수 토큰의 의미를 학습시킬 수 있다. 이 경우 누가 말할 차례인지와 같은 정보를 알 수 있다.
특수 토큰을 도입하는 방식은 구조화 된 문서를 단순한 1차원 배열로 바꿔 표현할 수 있다느 것이고, 이전 학습에 썼던 방법을 그대로 사용할 수 있다는 뜻이다.

### 학습 이후
앞으로 있을 모든 질문에 대해서 알지 못하더라도, 충분한 수위 대화 데이터셋만 있다면 데이터 레이블러들가 작성한 행동을 모방하여 
통계적으로 Assistant 성격의 행동을 학습하게 된다. 베이스 모델이 **인터넷 문서 시뮬레이터** 였다면, 사후 훈련 모델은 **인간 레이블러 시뮬레이터**이자
무작위 인간이 아닌 특정 분야에 맞게 학습한 **전문가 시뮬레이터**이다.





## LLM의 패러다임 전환 : RL기반의 추론 모델

### 강화 학습 (RL)의 가능성
지도 학습은 전문가가 답변한 내용을 "따라"할 뿐이기 때문에 아무리 학습을 반복해도 인류 최고의 전문가를 능가하기 어렵다고 한다. 반면에
강화 학습의 경우, 이세돌과 경기에서 알파고가 보여줬던 "37수"처럼 사람들이 생각하지 못한 이상적인 전략을 만들어 낼 수 있다. 이러한 관점으로 보았을 때,
기존 ChatGPT의 SFT 방식은 전문가의 답변을 따라할 뿐 그 이상을 넘기 힘들겠다는 생각을 할 수 있다. 최근 논란이 되었던 DeepSeek 모델은
이러한 방식에서 벗어나 순수 RL 기반으로 학습되어 기존 모델에 비해 우수한 성능을 낼 수 있었다.

### DeepSeek 모델의 학습 방법


### 검증하기 어려운 도메인에서의 강화학습
농담을 잘하는 모델을 만드는 예를 들어 정량적인 평가가 어려운 영역에 대해서는 강화학습을 적용하기 어렵다고 설명한다.
강화학습에는 수천 번의 업데이트가 필요하며, 한 업데이트 당 수천개의 프롬프트를 봐야하며, 한 프롬프트에는 수천개의 생성결과를 평가해야 하는데,
정량적인 평가가 어려운 영역에서는 사람이 일일이 평가를 해야하기 때문에 비용적인 문제가 커지켜 "확장 가능한" 방법이 아니다.

이를 위해 "인간 피드백 기반 강화학습(RLHF)"이 제시되었고, 대표적으로 Rlichev 접근법이 있다.

#### Rlichev Approach
이 접근법에서는 인간의 평가를 시뮬레이션하는 보상 모델을 만들어 강화학습에 대한 평가를 사람 대신 하도록 하는 접근법이다.
물론 초기에 보상 모델을 훈련시킬 때에는 인력이 필요하지만, 수차례의 학습 후에는 이 모델로 얼마든지 결과를 평가할 수 있기 때문에 확장성 측면에서 좋다.

구체적으로 사람으로 보상 모델을 훈련시킬 때에는 다음과 같은 과정을 거친다.
1. 생성결과들을 보고 점수로 표현하기 보다는 더 쉬운 평가 방식인 "등수"를 나타내도록 한다.
2. 보상 모델에게도 {프롬프트, 후보 결과}를 주고 점수를 출력하도록 한다. (0과 1 사이)
3. 보상 모델이 출력한 결과의 등수와 사람이 정한 등수를 비교한다.
4. 수학적인 방법으로 이 차이를 줄이도록 학습시킨다. 

#### RHLF의 장점
검증 불가능한 영역에 대해서도 강화학습을 적용할 수 있다. 실제로 적용을 하면 모델 성능이 나아지는 것을 알 수 있는데 정확히 왜 그런지는 밝혀지지 않았다고 한다. 보상 모델을 훈련할 비용으로 목표하는 모델을 훈련하는게 더 직접적인 도움이 되지 않나 싶긴 하다. 영상에서는 **생성기에 비해 판별기의 복잡도가 더 낮기** 때문이라고 추측한다. 
>  이러한 비슷한 예제로 임베딩 모델을 훈련할 때 Negative Sampling 방식으로 좀 더 효율적으로 학습시켰다는 논문을 떠올릴 수 있었다. 

#### RHLF의 한계
당연히 실제 인간의 판단이 아닌 단순화 된 시뮬레이션으로 강화학습 한다는 근본적인 문제가 있지만 더 미묘한 문제는 이 방식은 모델과 시뮬레이션 자체를 오염시킬 수 있다는 점이다. 단순히 생각하기로는 만들어진 보상 모델을 이용해 학습을 오래할수록 더 높은 품질의 답변을 생성할 것 같지만, 실제로 그렇지 않다고 한다. 보상 모델은 어떤 특정한 "적대적 예시"에 대해서 실제 사람과 전혀 다른 평가를 하게 되는데, 이로 인해 이상한 출력을 높게 평가하여 결과로 만들어진 모델이 이상한 출력을 내게 될 수 있다고 한다. 
> 영상에서 제시한 예시로는 펠리컨에 대한 농담으로 "the the the the the" 이런 문장을 보상 모델이 높은 점수를 주었다고 한다.

설령 그러한 적대적 예시들을 하나하나 발견하여 데이터셋에 추가하여 조정한다고 하더라도, 강화학습의 최적화에 의해 보상 모델의 빈틈을 파고들어 무한히 많은 적대적 입력들을 만들어낼 수 있기 때문이다. 
결국 이러한 리워드 모델을 너무 많이 사용하게 되면 강화학습이 보상 모델의 약점을 찾게 되기 때문에 무한히 사용할 수 없게 된다.

영상에서는 RHLF가 무한히 사용할 수 없다는 점을 들어 진정한 RL이 아닌 미세 조정의 일종이라고 본다.


## 모델의 미래 
### Multimodal : LLM 자체에서 audio, image, video를 지원하는 모델
Audio/Iamge/Video를 토큰화하여 입력으로 사용하는 식. 근본적인 변화가 있는 것이 아니라 토큰 종류가 증가하는 것
> 스펙트로그램, 이미지 패치 등등을 이용할 수 있다.

### Tasks -> Agent
작업 실행 구성을 우리가 하고, LLM이 실행하는 현재... 긴 작업을 위해 여러 과제를 연결하는 것이 아직 불가능함.
미래에는 이러한 작업을 몇 분에서 몇 시간까지 진행하는 Agent가 등장할 것이라고 한다. 하지만 마치 자동화된 공장에서 로봇들으 관리하는 직업이 있듯이 잘 하고 있는지 볼 수 있는 감독은 필요하다. 

### Test Time Training
지금까지는 매개변수 훈련 / 추론으로 두 가지 단계가 명확하게 나누어져 있음. 모델은 상황별로 조절 가능한 컨텍스트 윈도우를 통한 문맥 학습밖에 하지 못함. 추론 과정에서 매개변수를 조정할 수 없다는 점이 아쉽다. 특히 컨텍스트 윈도우 크기는 제한적일 수 밖에 없는데, 후에 비디오와 같이 토큰이 많이 필요한 입력이 들어오게 되면 활용할 수 있는 공간이 부족하게 된다.

### 최신 발전을 경험할 수 있는 곳
1. lmarena.ai : 사람 평가 기반의 LLM 순위표, 최근 몇 달 동안은 조작된 것 같아서 예전만큼은 신뢰하지 않음 (gemini, sonnet)
2. AI News letter : 포괄적인 내용들이 많아 좋다. 대부분은 AI가 작성함. 본문 내용은 많지만 요약본은 퀄리티가 좋음.
3. X/Twitter : 신뢰할 수 있는 사람들을 팔로우 하기

### AI 모델을 접근할 수 있는 곳
1. 모델 제공자 사이트
2. Together.ai
3. Hyperbolic : base model
4. LMStudio : 작은 로컬 모델 사용하기에 적합. DeepSeek 모델을 보통 사용함.
