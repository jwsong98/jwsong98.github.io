---
title: Deep Dive Into LLMs like ChatGPT 리뷰
date: 2025-04-18 13:55:17
categories: [Review, AI]
tags: [LLM, ChatGPT, DeepSeek]
---
### 개요
[(한글자막) 대형 언어 모델(LLM)의 심층 분석: ChatGPT의 작동 방식 이해하기](https://www.youtube.com/watch?v=6PTCwRRUHjE&t=10430s)

이 영상(3시간 반)을 보면서 배웠던 내용을 내 나름대로 정리해보려고 한다. 영상은 ChatGPT와 같은 LLM이 어떻게 만들어지는지, 어떻게 작동하는지, 어떤 약점을 가지는지 등 전반적인 내용에 대해서 알려준다. 구체적인 구현 사항들까지 다루진 않지만 적절한 비유와 함께 설명을 해주어 원리를 이해하는데 많은 도움이 되었다.

## 사전 훈련
### 데이터 셋 구성
대부분의 기업에서는 대량의 다양한 분야의 높은 질의 데이터셋을 구성하는 것을 목표로 한다. 이를 위해 많은 양의 데이터를 수집하기도 하지만, 전처리 과정이 필수적이라고 한다. 영상에서는 FineWeb 데이터셋을 예로 들어 설명하는데, 자세한 설명은 [FineWeb 데이터셋](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1)에서 볼 수 있다.대략적으로 다음과 같은 과정을 거친다고 한다.

#### FineWeb의 데이터 전처리 과정
1. URL Filtering : 성인사이트와 같은 부적절한 데이터 소스를 URL 단위로 거른다.
2. Text Extraction : HTML 문서에서 HTML 태그 정보를 제외한 텍스트 값들을 추출
3. Language Filtering : 언어 분류기를 통한 문서 내의 영어 비중을 판단하고 일정 비율 이상인 문서만 추출
4. 기타 필터링 : 개인 정보 제거 등등
> 영상에서 다뤘던 부분들만 짚고 넘어갔는데, 기타 필터링에 많은 작업들을 한다.

### 토큰화
이렇게 모은 데이터를 입력으로 하여 모델을 학습시켜야 하는데, 언어 모델은 한정된 입력 크기를 가진다. 
때문에 텍스트 데이터를 인코딩한 비트열을 그대로 사용하기에는 입력의 시퀀스의 길이가 필요 이상으로 길어지게 된다. 
표현할 수 있는 기호를 늘려 시퀀스의 길이를 줄일 수 있다. 이를 위한 다양한 알고리즘이 있지만, 영상에서는 **바이트 페어 인코딩**이라는 알고리즘을 소개한다. 
8비트를 묶은 한 바이트 단위로 표현하고, 여기서 빈번히 등장하는 바이트 페어를 묶어 새로운 토큰으로 만들어 시퀀스 길이를 줄인다. 
이런 식으로 토큰 집합의 크기를 늘려 GPT 4의 경우 100,277개의 토큰이 사용한다고 한다. 
GPT 4의 토큰나이저는 [Tiktokenizer](https://tiktokenizer.vercel.app/)의 cl100k_base를 통해 체험해 볼 수 있다. 이런 토큰화는 LLM이 우리의 예상과 다르게 동작하는 기반이 된다.

### 모델 학습
이렇게 토큰화 된 문서를 기반으로 다음 올 단어를 예측하는 언어모델을 만들어낸다. 일정한 크기의 컨텍스트 윈도우를 잡고 이 컨텍스트를 기반으로 다음에 올 토큰이 등장 확률 분포를
예측한다. 출력으로 토큰이 나오는 것이 아니라, 각 토큰이 나올 확률이 나온다. 실제로 문장을 만들 땐, 이 확률 분포에 따라 랜덤으로 토큰이 추출된다. 
이렇게 추출된 토큰을 기반으로 다시 다음 토큰의 확률분포를 만들어내기 때문에 운이 좋으면 원본 데이터를 만들어내지만, 원본과 완전히 다른 토큰열이 출력될 수도 있다.
> 더 잘 예측하게 하고, 더 효율적으로 학습하게 하기 위해서 RNN, LSTM, Transformer 등 다양한 모델들이 제안되었고, 현재는 Transformer 기반의 모델들이 지배적이다

수 많은 컴퓨팅 자원을 이용해 긴 시간 학습을 하고 나면 **토큰 시뮬레이터** 역할을 하는 **베이스 모델**을 얻을 수 있다. 

### 베이스 모델
베이스 모델은 영상에서 소개한 것처럼 **토큰 시뮬레이터** 역할을 한다. 인터넷 지식을 *상상*하며 다음에 올 토큰을 통계적으로 예측할 뿐이다. 
[Hyperbolic](https://app.hyperbolic.xyz/models)에서 베이스 모델을 테스트 해보면 입력한 문장을 계속해서 이어가기만 하는 것을 확인할 수 있다.
결과가 그렇게 유용해보이진 않지만, 다음 토큰을 예측하는 학습을 하면서 세상의 지식을 신경망 내부에 저장한다는 점에서 의미가 있다.
> 영상에서 베이스 모델을 **인터넷의 손실 압축본**, **인터넷 문서 시뮬레이터**이라고 표현하는데, 굉장히 와닿았다

프롬프트는 이 압축파일에서 지식을 꺼낼 수 있는 수단으로 활용된다. 베이스 모델에서는 마치 문자을 쓰다만 것처럼 프롬프틀 작성해서 프랑스의 대표적인 관광명소 
Top10에 대해서 지식을 얻어냈다. 이렇게 얻어낸 지식은 정확하지 않을 수 있으며 많이 나오는 지식일수록 더 잘 나오게 되는 특성을 가진다. 
실제로 위키피디아의 일부 내용을 프롬프트에 전달하면 위키피디아 내용을 거의 그대로 재생산하는 것을 볼 수 있는데, 위키피디아 데이터가 이곳저곳에서 
재사용되는 것에 기인한다. 기억효율이 인간보다 훨씬 좋은 LLM은 몇번만 보더라도 거의 같은 내용을 기억해 재현할 수 있다.

#### 문맥 내 학습
이러한 단점에도 프롬프트를 잘 설계하면 "문맥 내 학습" 능력으로 베이스 모델을 유용하게 활용할 수 있다. 대표적으로 **퓨샷 프롬프트**를 활용하면,
실시간으로 입력 프롬프트에 있는 패턴을 분석하여 내가 원하는 동작을 수행할 수 있게 한다.

## 사후 훈련 (SFT)
베이스 모델만으로는 우리의 질문에 알맞게 대답하는 Assistant 역할을 잘 해내기엔 어렵다. LLM을 Assistant 역할을 하게 하기 위해 사후 처리를 하게 되는데,
사전 훈련에 비해 훨씬 컴퓨팅 자원을 덜 소비하며 빠르게 진행된다. 질문에 대답하는 Assistant 대화의 흐름을 생각해보면, 2+2=?와 같은 질문에는 4라고 대답을 하고 해킹을 해달라고 하면
할 수 없다고도 대답한다. 이처럼 LLM에게 Assistant의 행동을 가르쳐야 하는데, LLM은 신경망일 뿐이므로 코드로는 그러한 방식을 가르칠 수 없다.
대신 행동에 맞는 예시를 만들어 학습시켜 간접적으로 가르칠 수 있다. 사전 훈련과 마찬가지 알고리즘을 사용하되 데이터셋만 바뀌는 식으로 진행된다.

### 데이터셋 확보
LLM에게 특정한 행동을 가르치기 위해 그 행동에 맞게 대답하는 실제 예시들이 필요하다. 많은 LLM 기업들은 적절한 지침을 데이터 레이블러에게 가르치고,
그 지침에 맞게 어떤 질문에 대한 대답을 인간 레이블러들이 작성하며 데이터를 확보한다. 이런 데이터셋을 통해 다시 한번 학습한 모델은 별도의 프롬프트 설계를 하지 않더라도
질문에 맞게 대답하게 된다.
> 최근에 들어서는 순수 사람이 만든 데이셋을 쓰기 보다 UltraChat과 같이 LLM의 도움을 받아 만들어진 합성 데이터셋을 사용해 구축하기도 한다. 

### 토큰화
"대화"와 "질문/대답"이라는 구조를 LLM에게 전달하기 위해 이런 구조를 반영한 토큰화가 필요하다. 기존 데이터에는 없는 특수 토큰을 추가하여 사용자의 말과 
Assistant의 말을 구분한다. 이러한 토큰을 도입하고 충분히 학습시켜 특수 토큰의 의미를 학습시킬 수 있다. 이 경우 누가 말할 차례인지와 같은 정보를 알 수 있다.
특수 토큰을 도입하는 방식은 구조화 된 문서를 단순한 1차원 배열로 바꿔 표현할 수 있다느 것이고, 이전 학습에 썼던 방법을 그대로 사용할 수 있다는 뜻이다.

### 학습 이후
앞으로 있을 모든 질문에 대해서 알지 못하더라도, 충분한 수위 대화 데이터셋만 있다면 데이터 레이블러들가 작성한 행동을 모방하여 
통계적으로 Assistant 성격의 행동을 학습하게 된다. 베이스 모델이 **인터넷 문서 시뮬레이터** 였다면, 사후 훈련 모델은 **인간 레이블러 시뮬레이터**이자
무작위 인간이 아닌 특정 분야에 맞게 학습한 **전문가 시뮬레이터**이다.

## LLM의 인지적 특징 

### LLM 환각 문제
SFT 데이터셋을 구성할 때 데이터 레이블러는 질문에 대한 대답을 아는 것이면 아는 것인대로, 모르면 인터넷 검색을 통해 나온대로 "자신감"있는 어조로 답변을 작성하게 된다.
그 결과로 만들어진 모델도 애매하다고 생각되는 질문에도 자신감 있는 어조로 랜덤한 답변을 생성하게 된다. 
> 실제로 베이스 모델을 테스트 해보면 같은 질문에 거의 항상 다른 대답을 할 정도로 토큰 확률 분포가 고르게 퍼져있다는 것을 알 수가 있는데 이는 곧 모델 스스로 출력이 애매하다는 
> 알고 있다고 생각할 수 있다.

#### 해결방법
- Meta에서는 데이터 셋에 "모른다"고 대답하는 대화를 추가하여 모델이 잘 알지 못하는 상태를 실제로 "모른다"고 표현하는 것으로 연결짓게 만들어 해결했다.
  - 모델이 "모르는" 질문은 다른 LLM에 컨텍스트에 정보를 넣어 질문을 만들어 대상이 되는 LLM에게 수차례 물어보는 과정을 통해 알 수 있다.
- 문맥 내 학습 능력을 활용하여, 잘 모르는 입력이 들어왔을 때 "검색 토큰"을 출력하게 하고 실제로 Bing 검색 엔진을 통해 나온 결과를 해당 부분에 삽입해 이를 바탕으로 처리하는 방법도 있다.

### 모델 자신에 대한 이해
사람들은 LLM에게 "너는 누구냐"라는 질문을 많이 하곤 하는데, 모델 자체에는 자아가 없는 토큰 생성기이기 때문에 대부분 통계적으로 많이 나온 단어로 대답하게 된다.
이러한 질문에 정확하게 대답하게 하기 위해서는 SFT 데이터셋에 자기 자신에 대한 질문과 대답을 추가하여 보강하는 방법이 있고, 프롬프트에 사용자의 입력과 같이 모델 정보를 넣어
LLM이 이를 활용할 수 있게 유도하는 방법이 있다.

### LLM 기본 연산 능력
LLM은 구조적으로 한 토큰을 만드는데 사용되는 연산 횟수가 **정해져**있고, 문장의 시작에서 끝방향으로만 토큰을 예측해나간다. 
따라서 원하는 출력에 많은 의미와 과정이 내포되어 있을수록, 그 토큰을 예측하기 어려워진다.
> 당연한 말을 자동완성 하는 작업과 매우 어려운 빈칸 채우기를 하는 작업을 같은 연산을 통해 계산한다고 생각해보면 이해가 된다.

따라서 지나치게 어려운 토큰을 한번에 만들어 내기 보다 여러차례 중간단계의 토큰을 만들고, 중간 단계의 토큰을 통해 어려운 토큰을 예측하게 하는 것이 도움이 된다.
> 영상에서 모델은 생각하는데 토큰이 필요하다고 설명하는데 위와 같은 특징을 살펴보면 일리가 있는 설명인 것 같다.

#### 후미식 데이터셋 VS 두괄식 데이터셋
이러한 LLM의 특징 때문에 데이터셋도 두괄식으로 답변을 작성하게 되면 제일 먼저 나오는 결론은 정확하지 않고, 뒤에 나오는 설명을 이를 **정당화**할 뿐이다.
반면에 후미식은 더 쉬운 중간과정을 예측하고, 이를 기반으로 어려운 결론을 도출하기 때문에 결론에 대한 설명을 제시할 수 있으면서도, 더 정확한 결론을 만들어 낼 수 있다.
> SFT단계에서 데이터 레이블러들이 이러한 특징을 고려했기 때문에 프롬프트를 작성하는 우리들은 그렇게 까지 신경쓸 필요는 없다고 한다.

#### 실시간 코드 실행을 이용한 연산 정확도 개선
아무리 중간단계를 거친다고 하더라도, 여전히 LLM은 다음에 올 토큰을 예측하는 식으로 작동하기 때문에 중간에 실수를 할 수 있다. 
> 영상에서는 마치 사람이 머릿속으로 수학문제를 암산을 하여 결과를 내는 것으로 빗대어 표현했는데 와닿게 잘 표현한 것 같다.

이러한 불확실성을 "코딩"를 통해 해결할 수 있다. 어떤 방정식을 푸는 python 코드를 모델이 작성하고 이를 다른 서버에 전달하여 실행 결과를 받고 그 결과를
사용자에게 다시 전달하여 정확도를 높일 수 있다. 

### LLM의 "세는" 연산 능력
GPT에게 "............................................." 의 점은 몇개인지 물어보면 박사급 문제를 푸는 GPT 조차 제대로 결과를 내지 못한다는 것을 알 수 있다.
이렇게 부정확한 결과를 내는 이유는 LLM은 문자열을 그대로 보지 않고, 토큰들로 보기 때문이다.(모델의 세는 능력도 자체도 약하다고 한다.) LLM은 각 토큰이 몇 개의 점들로 이루어져 있는지 명확하게 인지하지 못하기 때문에 이러한 일이 발생한다.
역시 이러한 문제도 중간 결과로 점의 개수를 세는 코드를 생성하고 이를 실행한 결과를 출력하게 하여 결과의 신뢰도를 높일 수 있다.
> LLM은 문법 문제에도 비슷한 이유로 약하고, 대소 비교 (9.11 vs 9.9)에도 약한 모습을 보인다.

## 강화학습
학교에서 무언갈 배울 때, 교과서에 있는 내용을 익히고 (사전 학습), 풀이가 있는 예제를 풀어보고 (SFT), 단원 뒤에 있는 답만 있는 연습 문제(강화 학습)를 풀곤 했다.
LLM도 이와 마찬가지로 학습이 진행되는 걸 확인할 수 있다.

### 무엇이 좋은 출력 인가?
우리가 수학 문제를 푸는 모델이 학습할 데이터셋을 만든다고 할 때, 어떤 풀이를 적어주어야 가장 좋을지 생각해보면 쉽게 선택을 할 수 없을 것 같다. 그도 그럴 것이
어떤 때에는 자세한 풀이가 좋을 것이고, 어떤 때에는 간결한 풀이가 좋을 것이고, 어떤 때에는 기발한 아이디어로 문제를 정말 짧게 푸는 것이 좋을 것이기 때문에 기준을 세우기가
어렵다.

LLM 학습에도 마찬가지의 문제가 있다. 어떤 추론의 난이도를 낮추기 위해서는 토큰을 많이 사용해야 할 것이지만, 생각보다 간단한 문제에 많은 토큰을 출력하게 하는 것은 낭비가 된다.
앞에서 보았듯이 LLM은 우리와 다른 방식으로 새상을 바라보기 때문에 인간은 쉽사리 LLM에게 쉬운 문제, 어려운 문제를 구분해주기 어렵다. 따라서 LLM 스스로 이것들을 조절할 필요가 있고,
이것을 학습하는 단계가 강화학습 단계이다.

이 단계에서 LLM은 정답을 맞추는 것 뿐만 아니라 사람에게 보기 좋게 출력하거나, 토큰을 적게 사용하는 방법 등 다른 추가적인 조건들을 고려해 가장 좋은 답변을 만들도록
학습된다. 하지만 실제로 훈련을 시킬 때에는 최적의 해결책을 찾는 방법이나 기준, 어떻게 피드백을 반영할 지 등등에 대한 구체적인 것들이 아직까지 표준화되지 않고 있다.

### DeepSeek 모델의 학습 방법
이러한 가운데 DeepSeek의 논문은 꽤 중요한 역할을 하게 된다. 여기서 모델은 스스로 답을 찾아가는 과정을 검토하고 최적화하고 평가하며 점점 더 최적해에 가까워진다. 
이것이 중요한 이유는 프로그래밍으로는 이러한 이상적인 답변을 유도하기 어렵고, 오로지 강화학습에서만 이러한 현상을 발견할 수 있기 때문이다. 
답만 주웠는데도 다양한 방식을 시도하며 문제를 이상적으로 푸는 방법을 학습할 수 있었다.

이렇게 학습된 모델을 "추론" 모델이라고 부른다.

### 강화 학습 (RL)의 가능성
지도 학습은 전문가가 답변한 내용을 "따라"할 뿐이기 때문에 아무리 학습을 반복해도 인류 최고의 전문가를 능가하기 어렵다고 한다. 반면에
강화 학습의 경우, 이세돌과 경기에서 알파고가 보여줬던 "37수"처럼 사람들이 생각하지 못한 이상적인 전략을 만들어 낼 수 있다. 이러한 관점으로 보았을 때,
기존 ChatGPT의 SFT 방식은 전문가의 답변을 따라할 뿐 그 이상을 넘기 힘들겠다는 생각을 할 수 있다. 최근 논란이 되었던 DeepSeek 모델은
이러한 방식에서 벗어나 순수 RL 기반으로 학습되어 기존 모델에 비해 더 **독창적인** 추론방식을 학습하여 우수한 성능을 낼 수 있었다.

최근 연구에서는 이러한 학습에 사용될 수 있는 품질 높은 연습문제를 만들기 위해 방대하고 다양한 프롬프트 분포를 만드는데 집중하고 있다.

### 검증하기 어려운 도메인에서의 강화학습
농담을 잘하는 모델을 만드는 예를 들어 정량적인 평가가 어려운 영역에 대해서는 강화학습을 적용하기 어렵다고 설명한다.
강화학습에는 수천 번의 업데이트가 필요하며, 한 업데이트 당 수천개의 프롬프트를 봐야하며, 한 프롬프트에는 수천개의 생성결과를 평가해야 하는데,
정량적인 평가가 어려운 영역에서는 사람이 일일이 평가를 해야하기 때문에 비용적인 문제가 커지켜 "확장 가능한" 방법이 아니다.
이를 위해 "인간 피드백 기반 강화학습(RLHF)"이 제시되었다.

#### RLHF
이 방법은 인간의 평가를 시뮬레이션하는 보상 모델을 만들어 강화학습에 대한 평가를 사람 대신 하도록 하는 것이다.
물론 초기에 보상 모델을 훈련시킬 때에는 인력이 필요하지만, 수차례의 학습 후에는 이 모델로 얼마든지 결과를 평가할 수 있기 때문에 확장성 측면에서 좋다.

구체적으로 사람으로 보상 모델을 훈련시킬 때에는 다음과 같은 과정을 거친다.
1. 생성결과들을 보고 점수로 표현하기 보다는 더 쉬운 평가 방식인 "등수"를 나타내도록 한다.
2. 보상 모델에게도 {프롬프트, 후보 결과}를 주고 점수를 출력하도록 한다. (0과 1 사이)
3. 보상 모델이 출력한 결과의 등수와 사람이 정한 등수를 비교한다.
4. 수학적인 방법으로 이 차이를 줄이도록 학습시킨다. 

#### RHLF의 장점
검증 불가능한 영역에 대해서도 강화학습을 적용할 수 있다. 실제로 적용을 하면 모델 성능이 나아지는 것을 알 수 있는데 정확히 왜 그런지는 밝혀지지 않았다고 한다. 보상 모델을 훈련할 비용으로 목표하는 모델을 훈련하는게 더 직접적인 도움이 되지 않나 싶긴 하다. 영상에서는 **생성기에 비해 판별기의 복잡도가 더 낮기** 때문이라고 추측한다. 
>  이러한 비슷한 예제로 임베딩 모델을 훈련할 때 Negative Sampling 방식으로 좀 더 효율적으로 학습시켰다는 논문을 떠올릴 수 있었다. 

#### RHLF의 한계
당연히 실제 인간의 판단이 아닌 단순화 된 시뮬레이션으로 강화학습 한다는 근본적인 문제가 있지만 더 미묘한 문제는 이 방식은 모델과 시뮬레이션 자체를 오염시킬 수 있다는 점이다. 단순히 생각하기로는 만들어진 보상 모델을 이용해 학습을 오래할수록 더 높은 품질의 답변을 생성할 것 같지만, 실제로 그렇지 않다고 한다. 보상 모델은 어떤 특정한 "적대적 예시"에 대해서 실제 사람과 전혀 다른 평가를 하게 되는데, 이로 인해 이상한 출력을 높게 평가하여 결과로 만들어진 모델이 이상한 출력을 내게 될 수 있다고 한다. 
> 영상에서 제시한 예시로는 펠리컨에 대한 농담으로 "the the the the the" 이런 문장을 보상 모델이 높은 점수를 주었다고 한다.

설령 그러한 적대적 예시들을 하나하나 발견하여 데이터셋에 추가하여 조정한다고 하더라도, 강화학습의 최적화에 의해 보상 모델의 빈틈을 파고들어 무한히 많은 적대적 입력들을 만들어낼 수 있기 때문이다. 
결국 이러한 리워드 모델을 너무 많이 사용하게 되면 강화학습이 보상 모델의 약점을 찾게 되기 때문에 무한히 사용할 수 없게 된다.

영상에서는 RHLF가 무한히 사용할 수 없다는 점을 들어 진정한 RL이 아닌 미세 조정의 일종이라고 본다.


## 모델의 미래 
### Multimodal
Audio/Image/Video를 토큰화하여 입력으로 사용하는 식. 근본적인 변화가 있는 것이 아니라 토큰 종류가 증가하는 식으로 발전할 것이라고도 제안한다.
> 스펙트로그램, 이미지 패치 등등을 이용할 수 있다.

### Tasks에서 Agent로
작업 실행 구성을 우리가 하고, LLM이 실행하는 현재... 긴 작업을 위해 여러 과제를 연결하는 것이 아직 불가능하다.
미래에는 이러한 작업을 몇 분에서 몇 시간까지 진행하는 Agent가 등장할 것이라고 한다. 
하지만 마치 자동화된 공장에서 로봇들을 관리하는 직업이 있듯이 잘 하고 있는지 볼 수 있는 감독은 필요하다. 

### Test Time Training
지금까지는 매개변수 훈련 / 추론으로 두 가지 단계가 명확하게 나누어져 있다. 모델은 상황별로 조절 가능한 컨텍스트 윈도우를 통한 문맥 학습밖에 하지 못한다.
추론 과정에서 매개변수를 조정할 수 없다는 점이 아쉽다. 특히 컨텍스트 윈도우 크기는 제한적일 수 밖에 없는데, 후에 비디오와 같이 토큰이 많이 필요한 입력이 들어오게 되면 활용할 수 있는 공간이 부족하게 된다.

### 참고 사이트들
1. lmarena.ai : 사람 평가 기반의 LLM 순위표, 최근 몇 달 동안은 조작된 것 같아서 예전만큼은 신뢰하지 않음
2. AI News letter : 포괄적인 내용들이 많아 좋다. 대부분은 AI가 작성함. 본문 내용은 많지만 요약본은 퀄리티가 좋음.
3. X/Twitter : 신뢰할 수 있는 사람들을 팔로우 하기 
4. Together.ai 
5. Hyperbolic : base model 
6. LMStudio : 작은 로컬 모델 사용하기에 적합. DeepSeek 모델을 보통 사용함.
